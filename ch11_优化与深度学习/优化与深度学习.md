- [优化与深度学习](#优化与深度学习)
  - [优化的目标](#优化的目标)
- [局部最小值](#局部最小值)
- [鞍点](#鞍点)
- [梯度消失](#梯度消失)

## 优化与深度学习

1. 对于深度学习问题，我们通常会先定义损失函数。有了损失函数，就可以使用优化算法来尝试最小化损失
2. 在优化中，损失函数通常被称为优化问题的目标函数，按照传统惯例，大多数优化算法都关注的是最小化。如果我们需要最大化目标，那么有一个简单的解决方案：在目标函数前加负号即可

### 优化的目标

1. 优化主要关注的是最小化目标，深度学习关注在给定有限数据量的情况下寻找合适的模型
2. 优化算法的目标函数通常是基于训练数据集的损失函数，因此优化的目标是减少训练误差；深度学习（或更广义地说，统计推断）的目标是减少泛化误差。为了实现后者，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合

风险与经验风险：经验风险是训练数据集的平均损失，而风险则是整个数据群的预期损失

这里关注优化算法在最小化目标函数中发挥效果，而不是模型的泛化误差，这里需要区分优化问题中的解析解和数值解

- 在深度学习中，大多数目标函数都很复杂，没有解析解，必须使用数值优化算法
- 数值优化算法中存在以下挑战：局部最小值；鞍点；梯度消失

## 局部最小值

- 深度学习模型的目标函数通常有许多局部最优解
- 当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数局部最优，而不是全局最优
- 只有一定程度的噪声可能会使参数跳出局部最小值
- 这是小批量随机梯度下降的有利特性之一，小批量上梯度的自然变化能够将参数从局部极小值中跳出

## 鞍点

除了局部最小值之外，鞍点(saddle point)是梯度消失的另一个原因，鞍点是指函数的所有梯度都消失，但这个位置既不是全局最小值也不是局部最小值，考虑`y=x^3`在x=0处

高维函数的鞍点甚至更加隐蔽

假设函数的输入是k维向量，输出是标量，因此其Hessian矩阵将有k个特征值，函数的解可能是局部最小值、局部最大值或函数梯度为0位置处的鞍点

- 当函数在零梯度位置处的Hessian矩阵的特征值全部为正值时，我们有该函数的局部最小值
- 当函数在零梯度位置处的Hessian矩阵的特征值全部为负值时，我们有该函数的局部最大值
- 当函数在零梯度位置处的Hessian矩阵的特征值为负值和正值时，我们有该函数的一个鞍点

对于高维度问题，至少部分特征值为负的可能性相当高。这使得鞍点比局部最小值更有可能出现

- 凸函数是Hessian函数的特征值永远不为负值的函数，不幸的是，大多数深度学习问题并不属于这一类。尽管如此，它还是研究优化算法的一个很好的工具

## 梯度消失

可能遇到的最隐蔽问题是梯度消失

- 引入relu激活函数，缓解了其它激活函数容易遇到的梯度消失问题



