- [Transformer](#transformer)
  - [模型](#模型)
  - [基于位置的前馈网络](#基于位置的前馈网络)
  - [残差连接与层规范化](#残差连接与层规范化)

# Transformer

- 在比较了CNN,RNN与自注意力，自注意力同时具有**并行计算**和**最短的最大路径长度**这两个优势，因此，使用自注意力来设计深度架构是很有吸引力的
- Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层
- 尽管Transformer最初是应用于在文本数据上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如语言、视觉、语音和强化学习领域

## 模型

1. Transformer的编码器和解码器是基于自注意力的模块叠加而成, 源（输入）序列和目标（输出）序列的嵌入（embedding）表示将加上位置编码（positional encoding），再分别输入到编码器和解码器中

![alt text](image.png)

2. Transformer的编码器是由多个相同的层叠加而成的，每个层都有两个子层$sublayer$, 多头注意力汇聚与基于位置的前馈网络(positionwise feed-forward network)
3. 在计算编码器的自注意力时，查询、键和值都来自前一个编码器层的输出
4. 受到残差网络的启发，每个子层都采用了残差连接（residual connection），在残差链接的加法计算完之后紧接着层规范化(layer normalization)
5. 在Transformer中，对于序列中任何位置的任何输入$x 1*d 向量$都要满足$x+sublayer(x) 1*d$
6. 因此，输入序列对应的每个位置，Transformer编码器都将输出一个d维的表示向量

1. Transformer解码器也是由多个相同的层叠加而成的，并且层中使用了残差连接和层规范化
2. 除了编码器中描述的两个子层之外，解码器还在这两个子层之间插入了第三个子层，称为编码器－解码器注意力（encoder-decoder attention）层
3. 在编码器－解码器注意力中，查询Q来自前一个解码器层的输出，而键K和值V来自整个编码器的输出
4. 在解码器自注意力(masked self-attention)中，查询、键和值都来自上一个解码器层的输出, 但是，解码器中的每个位置只能考虑该位置之前的所有位置,这种掩蔽（masked）注意力保留了自回归（auto-regressive）属性，确保预测仅依赖于已生成的输出词元

## 基于位置的前馈网络

嗯...是一个MLP

## 残差连接与层规范化



